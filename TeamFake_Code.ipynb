{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The deep learning based model"
      ],
      "metadata": {
        "id": "wyyIGtK95lus"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw48SbsR0WwK"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df = pd.read_csv('WELFake_Dataset.csv', engine='python', on_bad_lines='skip')\n",
        "title = df[df.columns[0]].values\n",
        "text = df[df.columns[1]].values\n",
        "label_col = df[df.columns[-1]]\n",
        "\n",
        "valid_labels_mask = label_col.apply(lambda x: isinstance(x, str) and x.isdigit() and (x == '0' or x == '1'))\n",
        "df_filtered = df[valid_labels_mask].copy()\n",
        "title = df_filtered[df_filtered.columns[0]].values\n",
        "text = df_filtered[df_filtered.columns[1]].values\n",
        "label = df_filtered[df_filtered.columns[-1]].values.astype(int) # Convert label to integer\n",
        "title = [str(item) if pd.notna(item) else '' for item in title]\n",
        "text = [str(item) if pd.notna(item) else '' for item in text]\n",
        "\n",
        "title_train, title_test, text_train, text_test, label_train, label_test = train_test_split(title, text, label, test_size=0.3, random_state=84)\n",
        "\n",
        "tokenizer1 = Tokenizer(num_words=100)\n",
        "tokenizer1.fit_on_texts(title_train)\n",
        "title_train = tokenizer1.texts_to_sequences(title_train)\n",
        "title_test = tokenizer1.texts_to_sequences(title_test)\n",
        "tokenizer2 = Tokenizer(num_words=30000)\n",
        "tokenizer2.fit_on_texts(text_train)\n",
        "text_train = tokenizer2.texts_to_sequences(text_train)\n",
        "text_test = tokenizer2.texts_to_sequences(text_test)\n",
        "title_train_pad = pad_sequences(title_train, maxlen=100, padding=\"post\")\n",
        "title_test_pad = pad_sequences(title_test, maxlen=100, padding=\"post\")\n",
        "text_train_pad = pad_sequences(text_train, maxlen=3000, padding=\"post\")\n",
        "text_test_pad = pad_sequences(text_test, maxlen=3000, padding=\"post\")\n",
        "\n",
        "title_input = Input(shape=(100,), name='title_input')\n",
        "text_input = Input(shape=(3000,), name='text_input')\n",
        "title_embedding = Embedding(input_dim=10000, output_dim=128, input_length=100)(title_input)\n",
        "title_lstm = LSTM(64)(title_embedding)\n",
        "text_embedding = Embedding(input_dim=30000, output_dim=128, input_length=3000)(text_input)\n",
        "text_lstm = LSTM(64)(text_embedding)\n",
        "merged = Concatenate()([title_lstm, text_lstm])\n",
        "dense = Dense(64, activation='relu')(merged)\n",
        "output = Dense(1, activation='sigmoid')(dense)\n",
        "model = tf.keras.Model(inputs=[title_input, text_input], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit([title_train_pad, text_train_pad], label_train, epochs=30, batch_size=32, validation_data=([title_test_pad, text_test_pad], label_test))\n",
        "\n",
        "test_loss, test_acc = model.evaluate([title_test_pad, text_test_pad], label_test)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM based model (Mistral 7B)"
      ],
      "metadata": {
        "id": "PUI-MxEo5y8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes-cuda110 bitsandbytes\n",
        "!pip install --upgrade transformers\n",
        "!pip install evaluate\n",
        "from huggingface_hub import login\n",
        "login(new_session=False)\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    num_labels=2,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "df = pd.read_csv(\"WELFake_Dataset.csv\")\n",
        "dataset = Dataset.from_pandas(df)\n",
        "def preprocess(example):\n",
        "    text = f\"Title: {example['title']}\\nText: {example['text']}\"\n",
        "    encoding = tokenizer(text, truncation=True, max_length=512)\n",
        "    encoding[\"label\"] = int(example[\"label\"])\n",
        "    return encoding\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(-1)\n",
        "    return accuracy.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-fake-news-lora\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "XmkmeMb952HW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}